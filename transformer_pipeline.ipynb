{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72e9415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "import numpy as np\n",
    "import os\n",
    "# libraries for Transformer\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import wandb\n",
    "import evaluate\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281f5d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51d8adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key=os.getenv(\"WANDB_API_KEY\")\n",
    "run_name=os.getenv(\"WANDB_RUN_NAME\")\n",
    "# for development mode, set to true, right now imcreating pipeline, so take a data portion\n",
    "testing= os.getenv('TESTING','False').lower() # optional \n",
    "dataset_name=os.getenv(\"DATASET_NAME\")\n",
    "\n",
    "if api_key is None or dataset_name is None:\n",
    "    raise ValueError ('please...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58ab483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\fakih\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfakihakhan96\u001b[0m (\u001b[33mfakihakhan96-nust\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3ad6485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML OPS\\training-pipeline\\src\\wandb\\run-20251003_140106-dqcbs4uc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fakihakhan96-nust/bert-tiny-training/runs/dqcbs4uc' target=\"_blank\">ml_ops_test</a></strong> to <a href='https://wandb.ai/fakihakhan96-nust/bert-tiny-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fakihakhan96-nust/bert-tiny-training' target=\"_blank\">https://wandb.ai/fakihakhan96-nust/bert-tiny-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fakihakhan96-nust/bert-tiny-training/runs/dqcbs4uc' target=\"_blank\">https://wandb.ai/fakihakhan96-nust/bert-tiny-training/runs/dqcbs4uc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run=wandb.init(project ='bert-tiny-training',name=run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02af89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf71cd0f8b09405994cb84566a82ce73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ML OPS\\training-pipeline\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\fakih\\.cache\\huggingface\\hub\\datasets--moizasghar--imdb-small-5k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62fe4d14f12b477abd6fa2752c7d5824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/3.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5766ac6156814c81a7f26092c373da2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/1.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc98d9b133be4e6c9c6bc3bf0f72f84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/3750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1bea00475884b54ac694642cc80b69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets= load_dataset(dataset_name)\n",
    "datasets = DatasetDict({\n",
    "'train':datasets['train'],\n",
    "'test':datasets['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70568cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size :3750\n"
     ]
    }
   ],
   "source": [
    "print(f\"data size :{datasets['train'].num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23c247d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f251cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if testing =='true':\n",
    "    datasets['train']=datasets['train'].shuffle(seed=42).select(range(10))\n",
    "    datasets['test']=datasets['test'].shuffle(seed=42).select(range(1))\n",
    "    # reducing datasize for testing pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b0f2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ad82af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples['text'],padding=\"max_length\",truncation=True,max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1cf1708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcf949bbb5c0443b9e9b85de6e56a06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5029282564b54627ab8e599d4f1a426d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_datasets=datasets.map(tokenize_fn,batched=True)\n",
    "encoded_datasets.set_format(type='torch',columns=['input_ids','attention_mask','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b2f24f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a270913997847e290a1a1da6052518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model= AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\",num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd7c25ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a9f23b0c044db39c99f9fdb11bdcda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ffe36f83414b67bc00fdc603dccca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy=evaluate.load(\"accuracy\")\n",
    "f1=evaluate.load(\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1907dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(sample):\n",
    "    logits,labels=sample\n",
    "    pred=np.argmax(logits,axis=-1)\n",
    "    acc=accuracy.compute(predictions=pred,references=labels)\n",
    "    f1_score=f1.compute(predictions=pred,references=labels,average='weighted')\n",
    "    return{\"accuracy\":acc[\"accuracy\"],\"f1\":f1_score(\"f1\")}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be7f9861",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arg=TrainingArguments(\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type='cosine', #help training\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=1,\n",
    "    output_dir=\"./results\",\n",
    "    report_to='wandb',\n",
    "    run_name=run_name,\n",
    "    save_total_limit=3\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbb6c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer =Trainer(model=model,args=training_arg,\n",
    "                 train_dataset=encoded_datasets['train'],\n",
    "                 eval_dataset=encoded_datasets['test'],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be3e0574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\ML OPS\\training-pipeline\\src\\wandb\\run-20251003_135222-5w6hvyp7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fakihakhan96-nust/huggingface/runs/5w6hvyp7' target=\"_blank\">ml_ops_test</a></strong> to <a href='https://wandb.ai/fakihakhan96-nust/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fakihakhan96-nust/huggingface' target=\"_blank\">https://wandb.ai/fakihakhan96-nust/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fakihakhan96-nust/huggingface/runs/5w6hvyp7' target=\"_blank\">https://wandb.ai/fakihakhan96-nust/huggingface/runs/5w6hvyp7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using EarlyStoppingCallback without load_best_model_at_end=True. Once training is finished, the best model will not be loaded automatically.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "EarlyStoppingCallback requires metric_for_best_model to be defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML OPS\\training-pipeline\\env\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML OPS\\training-pipeline\\env\\Lib\\site-packages\\transformers\\trainer.py:2578\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2576\u001b[39m grad_norm: Optional[\u001b[38;5;28mfloat\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2577\u001b[39m learning_rate = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2578\u001b[39m \u001b[38;5;28mself\u001b[39m.control = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcallback_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2580\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.eval_on_start:\n\u001b[32m   2581\u001b[39m     \u001b[38;5;28mself\u001b[39m._evaluate(trial, ignore_keys_for_eval, skip_scheduler=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML OPS\\training-pipeline\\env\\Lib\\site-packages\\transformers\\trainer_callback.py:506\u001b[39m, in \u001b[36mCallbackHandler.on_train_begin\u001b[39m\u001b[34m(self, args, state, control)\u001b[39m\n\u001b[32m    504\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001b[32m    505\u001b[39m     control.should_training_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m506\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_event\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mon_train_begin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML OPS\\training-pipeline\\env\\Lib\\site-packages\\transformers\\trainer_callback.py:556\u001b[39m, in \u001b[36mCallbackHandler.call_event\u001b[39m\u001b[34m(self, event, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_event\u001b[39m(\u001b[38;5;28mself\u001b[39m, event, args, state, control, **kwargs):\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         result = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlr_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    568\u001b[39m         \u001b[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001b[39;00m\n\u001b[32m    569\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\ML OPS\\training-pipeline\\env\\Lib\\site-packages\\transformers\\trainer_callback.py:752\u001b[39m, in \u001b[36mEarlyStoppingCallback.on_train_begin\u001b[39m\u001b[34m(self, args, state, control, **kwargs)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args.load_best_model_at_end:\n\u001b[32m    748\u001b[39m     logger.warning(\n\u001b[32m    749\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing EarlyStoppingCallback without load_best_model_at_end=True. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    750\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOnce training is finished, the best model will not be loaded automatically.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    751\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m752\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m args.metric_for_best_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[32m    753\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEarlyStoppingCallback requires metric_for_best_model to be defined\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    754\u001b[39m )\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m args.eval_strategy != IntervalStrategy.NO, (\n\u001b[32m    756\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mEarlyStoppingCallback requires IntervalStrategy of steps or epoch\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    757\u001b[39m )\n",
      "\u001b[31mAssertionError\u001b[39m: EarlyStoppingCallback requires metric_for_best_model to be defined"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c989cd8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bert-tiny-sentiment\\\\tokenizer_config.json',\n",
       " 'bert-tiny-sentiment\\\\special_tokens_map.json',\n",
       " 'bert-tiny-sentiment\\\\vocab.txt',\n",
       " 'bert-tiny-sentiment\\\\added_tokens.json',\n",
       " 'bert-tiny-sentiment\\\\tokenizer.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label={0:'negative',1:'positive'}\n",
    "model.config.label2id={'negative':0,'positive':1}\n",
    "model.save_pretrained(\"bert-tiny-sentiment\")\n",
    "tokenizer.save_pretrained('bert-tiny-sentiment')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_env_training",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
